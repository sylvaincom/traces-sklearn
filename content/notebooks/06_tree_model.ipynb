{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "982802f2",
   "metadata": {},
   "source": [
    "\n",
    "# Decision trees\n",
    "\n",
    "We explore a class of algorithms based on decision trees. Decision trees are\n",
    "extremely intuitive at their core. They encode a series of \"if\" and \"else\" choices,\n",
    "similar to how a person makes a decision. The data determines which questions to ask\n",
    "and how to proceed for each answer.\n",
    "\n",
    "For example, to create a guide for identifying an animal found in nature,\n",
    "you might ask the following series of questions:\n",
    "\n",
    "- Is the animal bigger or smaller than a meter long?\n",
    "    + *bigger*: does the animal have horns?\n",
    "        - *yes*: are the horns longer than ten centimeters?\n",
    "        - *no*: does the animal wear a collar?\n",
    "    + *smaller*: does the animal have two or four legs?\n",
    "        - *two*: does the animal have wings?\n",
    "        - *four*: does the animal have a bushy tail?\n",
    "\n",
    "And so on. This binary splitting of questions forms the essence of a decision tree.\n",
    "\n",
    "Tree-based models offer several key benefits. First, they require little preprocessing\n",
    "of the data. They work with variables of different types (continuous and discrete)\n",
    "and remain invariant to feature scaling.\n",
    "\n",
    "Tree-based models are also \"nonparametric\", meaning they do not have a fixed set of\n",
    "parameters to learn. Instead, a tree model becomes more flexible with more data. In\n",
    "other words, the number of free parameters grows with the number of samples and is not\n",
    "fixed like in linear models.\n",
    "\n",
    "## Decision tree for classification\n",
    "\n",
    "Let's get some intuitions on how a decision tree works on a very simple dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca09a156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When using JupyterLite, uncomment and install the `skrub` package.\n",
    "%pip install skrub\n",
    "import matplotlib.pyplot as plt\n",
    "import skrub\n",
    "\n",
    "skrub.patch_display()  # makes nice display for pandas tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3379630",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples=100, centers=[[0, 0], [1, 1]], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86acb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "X = pd.DataFrame(X, columns=[\"Feature #0\", \"Feature #1\"])\n",
    "class_names = np.array([\"class #0\", \"class #1\"])\n",
    "y = pd.Series(class_names[y], name=\"Classes\").astype(\"category\")\n",
    "data = pd.concat([X, y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791aa804",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot.scatter(\n",
    "    x=\"Feature #0\",\n",
    "    y=\"Feature #1\",\n",
    "    c=\"Classes\",\n",
    "    s=50,\n",
    "    edgecolor=\"black\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2937ca44",
   "metadata": {},
   "source": [
    "\n",
    "Now, we train a decision tree classifier on this dataset. We first split the data\n",
    "into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a38d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_test, X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, X, y, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac398caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=1)\n",
    "tree.fit(X_train, y_train)\n",
    "pred = tree.predict(X_test)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f0bc01",
   "metadata": {},
   "source": [
    "\n",
    "We plot the decision boundaries found using the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b31df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "display = DecisionBoundaryDisplay.from_estimator(tree, X_train, alpha=0.7)\n",
    "data_train.plot.scatter(\n",
    "    x=\"Feature #0\", y=\"Feature #1\", c=\"Classes\", s=50, edgecolor=\"black\", ax=display.ax_\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62d6471",
   "metadata": {},
   "source": [
    "\n",
    "Similarly, we get the following classification on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1b392c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display = DecisionBoundaryDisplay.from_estimator(tree, X_test, alpha=0.7)\n",
    "data_test.plot.scatter(\n",
    "    x=\"Feature #0\", y=\"Feature #1\", c=\"Classes\", s=50, edgecolor=\"black\", ax=display.ax_\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568cad4f",
   "metadata": {},
   "source": [
    "We see that the decision found with a decision tree is a simple binary split.\n",
    "\n",
    "We can also plot the tree structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2178f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "plot_tree(tree, feature_names=X.columns, class_names=class_names, filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305292b1",
   "metadata": {},
   "source": [
    "\n",
    "**EXERCISE**\n",
    "\n",
    "1. Modify the depth of the tree and observe how the partitioning evolves.\n",
    "2. What can you conclude about under- and over-fitting of the tree model?\n",
    "3. How would you choose the best depth?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35faa467",
   "metadata": {},
   "source": [
    "\n",
    "Many parameters control the complexity of a tree, but maximum depth is perhaps the\n",
    "easiest to understand. This parameter limits how finely the tree can partition the\n",
    "input space, or how many \"if-else\" questions it can ask before deciding which class\n",
    "a sample belongs to.\n",
    "\n",
    "This parameter is crucial to tune for trees and tree-based models. The interactive\n",
    "plot below shows how underfitting and overfitting look for this model. A\n",
    "``max_depth`` of 1 clearly underfits the model, while a depth of 7 or 8 clearly\n",
    "overfits. The maximum possible depth for this dataset is 8, at which point each leaf\n",
    "contains samples from only a single class. We call these leaves \"pure.\"\n",
    "\n",
    "In the interactive plot below, blue and red colors indicate the predicted class for\n",
    "each region. The shade of color indicates the predicted probability for that class\n",
    "(darker = higher probability), while yellow regions indicate equal predicted\n",
    "probability for either class.\n",
    "\n",
    "### Note about partitioning in decision trees\n",
    "\n",
    "In this section, we examine in more detail how a tree selects the best partition.\n",
    "First, we use a real dataset instead of synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87bb866",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"../datasets/penguins.csv\")\n",
    "dataset = dataset.dropna(subset=[\"Body Mass (g)\"])\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bb93b7",
   "metadata": {},
   "source": [
    "\n",
    "We build a decision tree to classify penguin species using their body mass\n",
    "as a feature. To simplify the problem, we focus only on the Adelie and Gentoo species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed0fd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only select the column of interest\n",
    "dataset = dataset[[\"Body Mass (g)\", \"Species\"]]\n",
    "# Make the species name more readable\n",
    "dataset[\"Species\"] = dataset[\"Species\"].apply(lambda x: x.split()[0])\n",
    "# Only select the Adelie and Gentoo penguins\n",
    "dataset = dataset.set_index(\"Species\").loc[[\"Adelie\", \"Gentoo\"], :]\n",
    "# Sort all penguins by their body mass\n",
    "dataset = dataset.sort_values(by=\"Body Mass (g)\")\n",
    "# Convert the dataframe (2D) to a series (1D)\n",
    "dataset = dataset.squeeze()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873bf186",
   "metadata": {},
   "source": [
    "\n",
    "First, we examine the body mass distribution for each species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70554114",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "dataset.groupby(\"Species\").plot.hist(ax=ax, alpha=0.7, legend=True)\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debb41ad",
   "metadata": {},
   "source": [
    "\n",
    "Instead of looking at the distribution, we can look at all samples directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17144d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "ax = sns.swarmplot(x=dataset.values, y=[\"\"] * len(dataset), hue=dataset.index)\n",
    "ax.set_xlabel(dataset.name)\n",
    "ax.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b052330c",
   "metadata": {},
   "source": [
    "\n",
    "When we build a tree, we want to find splits that partition the data into groups that\n",
    "are as \"unmixed\" as possible. Let's make a first completely random split to highlight\n",
    "the principle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f82d941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a random state so we all get the same results\n",
    "rng = np.random.default_rng(42)\n",
    "random_idx = rng.choice(dataset.size)\n",
    "\n",
    "ax = sns.swarmplot(x=dataset.values, y=[\"\"] * len(dataset), hue=dataset.index)\n",
    "ax.set_xlabel(dataset.name)\n",
    "ax.set_title(f\"Body mass threshold: {dataset.iloc[random_idx]} grams\")\n",
    "ax.vlines(dataset.iloc[random_idx], -1, 1, color=\"red\", linestyle=\"--\")\n",
    "ax.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5723d73c",
   "metadata": {},
   "source": [
    "\n",
    "After the split, we want two partitions where samples come from a single class as\n",
    "much as possible and contain as many samples as possible. Decision trees use a\n",
    "**criterion** to assess split quality. **Entropy** describes the class mixture in a\n",
    "partition. Let's compute the entropy for the full dataset and the sets on each side\n",
    "of the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1ed935",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "dataset.index.value_counts()\n",
    "\n",
    "parent_entropy = entropy(dataset.index.value_counts(normalize=True))\n",
    "parent_entropy\n",
    "\n",
    "left_entropy = entropy(dataset[:random_idx].index.value_counts(normalize=True))\n",
    "left_entropy\n",
    "\n",
    "right_entropy = entropy(dataset[random_idx:].index.value_counts(normalize=True))\n",
    "right_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8767663d",
   "metadata": {},
   "source": [
    "\n",
    "We assess split quality by combining the entropies. This is called the\n",
    "**information gain**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace84594",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_entropy - (left_entropy + right_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044fedd4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "However, we should normalize the entropies by the number of samples in each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3ab2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(labels_parent, labels_left, labels_right):\n",
    "    # compute the entropies\n",
    "    entropy_parent = entropy(labels_parent.value_counts(normalize=True))\n",
    "    entropy_left = entropy(labels_left.value_counts(normalize=True))\n",
    "    entropy_right = entropy(labels_right.value_counts(normalize=True))\n",
    "\n",
    "    n_samples_parent = labels_parent.size\n",
    "    n_samples_left = labels_left.size\n",
    "    n_samples_right = labels_right.size\n",
    "\n",
    "    # normalize with the number of samples\n",
    "    normalized_entropy_left = (n_samples_left / n_samples_parent) * entropy_left\n",
    "    normalized_entropy_right = (n_samples_right / n_samples_parent) * entropy_right\n",
    "\n",
    "    return entropy_parent - normalized_entropy_left - normalized_entropy_right\n",
    "\n",
    "\n",
    "information_gain(dataset.index, dataset[:random_idx].index, dataset[random_idx:].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b013d9",
   "metadata": {},
   "source": [
    "\n",
    "Now we compute the information gain for all possible body mass thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1428869",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_information_gain = pd.Series(\n",
    "    [\n",
    "        information_gain(dataset.index, dataset[:idx].index, dataset[idx:].index)\n",
    "        for idx in range(dataset.size)\n",
    "    ],\n",
    "    index=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2b205b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = all_information_gain.plot()\n",
    "ax.set_ylabel(\"Information gain\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a353442",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = (all_information_gain * -1).plot(color=\"red\", label=\"Information gain\")\n",
    "ax = sns.swarmplot(x=dataset.values, y=[\"\"] * len(dataset), hue=dataset.index)\n",
    "ax.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f1e939",
   "metadata": {},
   "source": [
    "\n",
    "The maximum information gain corresponds to the split that best partitions our data.\n",
    "Let's check the corresponding body mass threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155a9ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_information_gain.idxmax()\n",
    "\n",
    "ax = (all_information_gain * -1).plot(color=\"red\", label=\"Information gain\")\n",
    "ax = sns.swarmplot(x=dataset.values, y=[\"\"] * len(dataset), hue=dataset.index)\n",
    "ax.vlines(all_information_gain.idxmax(), -1, 1, color=\"red\", linestyle=\"--\")\n",
    "ax.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dcff0e",
   "metadata": {},
   "source": [
    "\n",
    "## Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13e3ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd = np.random.default_rng(42)\n",
    "x = np.linspace(-3, 3, 100)\n",
    "y_no_noise = np.sin(4 * x) + x\n",
    "y = y_no_noise + rnd.normal(size=len(x))\n",
    "X = x.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d18cbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "ax.scatter(X, y, s=50)\n",
    "ax.set(xlabel=\"Feature X\", ylabel=\"Target y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb93e00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "reg = DecisionTreeRegressor(max_depth=2)\n",
    "reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d8bea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.linspace(-3, 3, 1000).reshape((-1, 1))\n",
    "y_test = reg.predict(X_test)\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "ax.plot(X_test.ravel(), y_test, color=\"tab:blue\", label=\"prediction\")\n",
    "ax.plot(X.ravel(), y, \"C7.\", label=\"training data\")\n",
    "ax.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06acf9a5",
   "metadata": {},
   "source": [
    "\n",
    "A single decision tree estimates the signal non-parametrically but has some issues.\n",
    "In some regions, the model shows high bias and underfits the data\n",
    "(seen in long flat lines that don't follow data contours),\n",
    "while in other regions it shows high variance and overfits\n",
    "(seen in narrow spikes influenced by noise in single points).\n",
    "\n",
    "**EXERCISE**\n",
    "\n",
    "1. Take the above example and repeat the training/testing by changing the tree depth.\n",
    "2. What can you conclude?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eafedcf",
   "metadata": {},
   "source": [
    "\n",
    "## Other tree hyperparameters\n",
    "\n",
    "The max_depth hyperparameter controls overall tree complexity. This parameter works\n",
    "well when a tree is symmetric. However, trees are not guaranteed to be symmetric. In\n",
    "fact, optimal generalization might require some branches to grow deeper than others.\n",
    "\n",
    "We build a dataset to illustrate this asymmetry. We generate a dataset with 2 subsets:\n",
    "one where the tree should find a clear separation and another where samples from both\n",
    "classes mix. This means a decision tree needs more splits to properly classify samples\n",
    "from the second subset than from the first subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7489ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "feature_names = [\"Feature #0\", \"Feature #1\"]\n",
    "target_name = \"Class\"\n",
    "\n",
    "# Blobs that will be interlaced\n",
    "X_1, y_1 = make_blobs(n_samples=300, centers=[[0, 0], [-1, -1]], random_state=0)\n",
    "# Blobs that will be easily separated\n",
    "X_2, y_2 = make_blobs(n_samples=300, centers=[[3, 6], [7, 0]], random_state=0)\n",
    "\n",
    "X = np.concatenate([X_1, X_2], axis=0)\n",
    "y = np.concatenate([y_1, y_2])\n",
    "data = np.concatenate([X, y[:, np.newaxis]], axis=1)\n",
    "data = pd.DataFrame(data, columns=feature_names + [target_name])\n",
    "data[target_name] = data[target_name].astype(np.int64).astype(\"category\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57daf694",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(10, 8))\n",
    "data.plot.scatter(\n",
    "    x=\"Feature #0\",\n",
    "    y=\"Feature #1\",\n",
    "    c=\"Class\",\n",
    "    s=100,\n",
    "    edgecolor=\"black\",\n",
    "    ax=ax,\n",
    ")\n",
    "ax.set_title(\"Synthetic dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acebd99f",
   "metadata": {},
   "source": [
    "\n",
    "First, we train a shallow decision tree with max_depth=2. This depth should suffice\n",
    "to separate the easily separable blobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d69b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 2\n",
    "tree = DecisionTreeClassifier(max_depth=max_depth)\n",
    "tree.fit(X, y)\n",
    "\n",
    "_, ax = plt.subplots(figsize=(10, 8))\n",
    "DecisionBoundaryDisplay.from_estimator(tree, X, cmap=plt.cm.RdBu, ax=ax)\n",
    "data.plot.scatter(\n",
    "    x=\"Feature #0\",\n",
    "    y=\"Feature #1\",\n",
    "    c=\"Class\",\n",
    "    s=100,\n",
    "    edgecolor=\"black\",\n",
    "    ax=ax,\n",
    ")\n",
    "ax.set_title(f\"Decision tree with max-depth of {max_depth}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbaa78e",
   "metadata": {},
   "source": [
    "\n",
    "As expected, the blue blob on the right and red blob on top separate easily. However,\n",
    "we need more splits to better separate the mixed blobs.\n",
    "\n",
    "The red blob on top and blue blob on the right separate perfectly. However, the tree\n",
    "still makes mistakes where the blobs mix. Let's examine the tree structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a9f863",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(15, 8))\n",
    "plot_tree(\n",
    "    tree, feature_names=feature_names, class_names=class_names, filled=True, ax=ax\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59258a2",
   "metadata": {},
   "source": [
    "\n",
    "The right branch achieves perfect classification. Now we increase the depth to see\n",
    "how the tree grows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc28d021",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 6\n",
    "tree = DecisionTreeClassifier(max_depth=max_depth)\n",
    "tree.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1c0d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(10, 8))\n",
    "DecisionBoundaryDisplay.from_estimator(tree, X, cmap=plt.cm.RdBu, ax=ax)\n",
    "data.plot.scatter(\n",
    "    x=\"Feature #0\",\n",
    "    y=\"Feature #1\",\n",
    "    c=\"Class\",\n",
    "    s=100,\n",
    "    edgecolor=\"black\",\n",
    "    ax=ax,\n",
    ")\n",
    "ax.set_title(f\"Decision tree with max-depth of {max_depth}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145168bd",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(25, 15))\n",
    "plot_tree(\n",
    "    tree, feature_names=feature_names, class_names=class_names, filled=True, ax=ax\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c127d765",
   "metadata": {},
   "source": [
    "\n",
    "As expected, the left branch continues to grow while the right branch stops splitting.\n",
    "Setting max_depth cuts the tree horizontally at a specific level, whether or not a\n",
    "branch would benefit from growing further.\n",
    "\n",
    "The hyperparameters min_samples_leaf, min_samples_split, max_leaf_nodes, and\n",
    "min_impurity_decrease allow asymmetric trees and apply constraints at the leaf or\n",
    "node level. Let's examine the effect of min_samples_leaf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b874d018",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples_leaf = 20\n",
    "tree = DecisionTreeClassifier(min_samples_leaf=min_samples_leaf)\n",
    "tree.fit(X, y)\n",
    "\n",
    "_, ax = plt.subplots(figsize=(10, 8))\n",
    "DecisionBoundaryDisplay.from_estimator(tree, X, cmap=plt.cm.RdBu, ax=ax)\n",
    "data.plot.scatter(\n",
    "    x=\"Feature #0\",\n",
    "    y=\"Feature #1\",\n",
    "    c=\"Class\",\n",
    "    s=100,\n",
    "    edgecolor=\"black\",\n",
    "    ax=ax,\n",
    ")\n",
    "ax.set_title(f\"Decision tree with leaf having at least {min_samples_leaf} samples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1f853f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(15, 15))\n",
    "plot_tree(\n",
    "    tree, feature_names=feature_names, class_names=class_names, filled=True, ax=ax\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57f4c10",
   "metadata": {},
   "source": [
    "\n",
    "This hyperparameter ensures leaves contain a minimum number of samples and prevents\n",
    "further splits otherwise. These hyperparameters offer an alternative to the max_depth\n",
    "parameter."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
