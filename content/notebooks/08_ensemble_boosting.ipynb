{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24d26a2b",
   "metadata": {},
   "source": [
    "\n",
    "# Boosted trees\n",
    "\n",
    "This notebook presents a second family of ensemble methods known as\n",
    "boosting. We first give an intuitive example of how boosting works, followed\n",
    "by an introduction to gradient boosting decision tree models.\n",
    "\n",
    "## Introduction to boosting\n",
    "\n",
    "We start with an intuitive explanation of the boosting principle. In\n",
    "the previous notebook, we saw that bagging creates several datasets with\n",
    "small variations using bootstrapping. An estimator trains on each\n",
    "dataset and aggregates the different results. In boosting, the paradigm differs:\n",
    "the estimators train on the same dataset. To combine them, each estimator\n",
    "corrects the error of all previous estimators. This creates a sequence of\n",
    "estimators instead of independent ones.\n",
    "\n",
    "Let's examine an example on a classification dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee49043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When using JupyterLite, uncomment and install the `skrub` package.\n",
    "%pip install skrub\n",
    "import matplotlib.pyplot as plt\n",
    "import skrub\n",
    "\n",
    "skrub.patch_display()  # makes nice display for pandas tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796acd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"../datasets/penguins_classification.csv\")\n",
    "data[\"Species\"] = data[\"Species\"].astype(\"category\")\n",
    "X, y = data[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]], data[\"Species\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b966a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(8, 6))\n",
    "data.plot.scatter(\n",
    "    x=\"Culmen Length (mm)\",\n",
    "    y=\"Culmen Depth (mm)\",\n",
    "    c=\"Species\",\n",
    "    edgecolor=\"black\",\n",
    "    s=80,\n",
    "    ax=ax,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a0af32",
   "metadata": {},
   "source": [
    "\n",
    "In this dataset, we distinguish three penguin species based on their culmen\n",
    "depth and length. We start by training a shallow decision tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cce3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=2, random_state=0)\n",
    "tree.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c5e672",
   "metadata": {},
   "source": [
    "\n",
    "We check the statistical performance of our model qualitatively by\n",
    "examining the decision boundary and highlighting misclassified samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3819da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "target_predicted = tree.predict(X)\n",
    "mask_misclassified = y != target_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48791ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "_, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "display = DecisionBoundaryDisplay.from_estimator(\n",
    "    tree, X, response_method=\"predict\", cmap=plt.cm.viridis, alpha=0.4, ax=ax\n",
    ")\n",
    "\n",
    "data.plot.scatter(\n",
    "    x=\"Culmen Length (mm)\",\n",
    "    y=\"Culmen Depth (mm)\",\n",
    "    c=\"Species\",\n",
    "    s=80,\n",
    "    edgecolor=\"black\",\n",
    "    alpha=0.5,\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "data[mask_misclassified].plot.scatter(\n",
    "    x=\"Culmen Length (mm)\",\n",
    "    y=\"Culmen Depth (mm)\",\n",
    "    s=200,\n",
    "    marker=\"+\",\n",
    "    color=\"tab:orange\",\n",
    "    linewidth=3,\n",
    "    ax=ax,\n",
    "    label=\"Misclassified samples\",\n",
    ")\n",
    "ax.legend()\n",
    "ax.set_title(\"Decision tree predictions \\nwith misclassified samples highlighted\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc8c159",
   "metadata": {},
   "source": [
    "\n",
    "Our decision tree makes several errors for some Gentoo and Adelie samples.\n",
    "Next, we train a new decision tree that focuses only on the misclassified\n",
    "samples. Scikit-learn's `fit` method includes a `sample_weight` parameter\n",
    "that gives more weight to specific samples. We use this parameter to focus\n",
    "our new decision tree on the misclassified samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114aae4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weight = mask_misclassified.astype(np.float64)\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=2, random_state=0)\n",
    "tree.fit(X, y, sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242d2178",
   "metadata": {},
   "source": [
    "\n",
    "Let's examine the decision boundary of this newly trained decision tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9959e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "display = DecisionBoundaryDisplay.from_estimator(\n",
    "    tree, X, response_method=\"predict\", cmap=plt.cm.viridis, alpha=0.4, ax=ax\n",
    ")\n",
    "\n",
    "data.plot.scatter(\n",
    "    x=\"Culmen Length (mm)\",\n",
    "    y=\"Culmen Depth (mm)\",\n",
    "    c=\"Species\",\n",
    "    s=80,\n",
    "    edgecolor=\"black\",\n",
    "    alpha=0.5,\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "data[mask_misclassified].plot.scatter(\n",
    "    x=\"Culmen Length (mm)\",\n",
    "    y=\"Culmen Depth (mm)\",\n",
    "    s=80,\n",
    "    marker=\"+\",\n",
    "    color=\"tab:orange\",\n",
    "    linewidth=3,\n",
    "    ax=ax,\n",
    "    label=\"Misclassified samples\",\n",
    ")\n",
    "ax.legend()\n",
    "ax.set_title(\"Decision tree predictions \\nwith misclassified samples highlighted\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dc9dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_predicted = tree.predict(X)\n",
    "mask_new_misclassifier = y != target_predicted\n",
    "remaining_misclassified_samples_idx = mask_misclassified & mask_new_misclassifier\n",
    "\n",
    "print(\n",
    "    f\"Number of samples previously misclassified and \"\n",
    "    f\"still misclassified: {remaining_misclassified_samples_idx.sum()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ef5b01",
   "metadata": {},
   "source": [
    "\n",
    "The previously misclassified samples now classify correctly. However, this\n",
    "improvement misclassifies other samples. We could continue training more\n",
    "decision tree classifiers, but we need a way to combine them. One approach\n",
    "weights each classifier based on its accuracy on the full training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfaab70",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_weight = [\n",
    "    (y.size - mask_misclassified.sum()) / y.size,\n",
    "    (y.size - mask_new_misclassifier.sum()) / y.size,\n",
    "]\n",
    "ensemble_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca87115",
   "metadata": {},
   "source": [
    "\n",
    "In our example, the first classification achieves good accuracy, so we trust\n",
    "it more than the second classifier. This suggests making a linear combination\n",
    "of the different decision tree classifiers.\n",
    "\n",
    "This example simplifies an algorithm known as `AdaBoostClassifier`.\n",
    "\n",
    "**EXERCISE:**\n",
    "\n",
    "1. Train a `sklearn.ensemble.AdaBoostClassifier` with 3 estimators and\n",
    "   a base `DecisionTreeClassifier` with `max_depth=3`.\n",
    "2. Access the fitted attribute `estimators_` containing the decision\n",
    "   tree classifiers and plot their decision boundaries.\n",
    "3. Find the weights associated with each decision tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3496d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c9931f",
   "metadata": {},
   "source": [
    "\n",
    "## Gradient Boosting Decision Trees\n",
    "\n",
    "AdaBoost predictors see less use today. Instead, gradient boosting decision\n",
    "trees demonstrate superior performance.\n",
    "\n",
    "In gradient boosting, each estimator uses a decision tree regressor even for\n",
    "classification. Regression trees provide continuous residuals. Each new\n",
    "estimator trains on the residuals of previous estimators. Parameters control\n",
    "how quickly the model corrects these residuals.\n",
    "\n",
    "Let's demonstrate this model on a classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82406b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv(\"../datasets/adult-census-numeric-all.csv\")\n",
    "X, y = data.drop(columns=\"class\"), data[\"class\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b592ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "classifier = GradientBoostingClassifier(n_estimators=5)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e59da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c93a63",
   "metadata": {},
   "source": [
    "\n",
    "Let's inspect the underlying estimators to confirm our use of decision\n",
    "tree regressors in this classification setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7cc80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.estimators_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031cfb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "_, ax = plt.subplots(figsize=(20, 8))\n",
    "\n",
    "plot_tree(\n",
    "    classifier.estimators_[0][0],\n",
    "    feature_names=X_train.columns,\n",
    "    ax=ax,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d353901b",
   "metadata": {},
   "source": [
    "\n",
    "## Histogram gradient boosting decision trees\n",
    "\n",
    "**EXERCISE:** Accelerate gradient boosting\n",
    "\n",
    "What solutions accelerate the training speed of gradient boosting algorithms?\n",
    "\n",
    "\n",
    "### Short introduction to `KBinsDiscretizer`\n",
    "\n",
    "Here's a trick to accelerate gradient boosting and decision trees in general.\n",
    "Decision trees choose splits from all unique values in a feature. Binning\n",
    "feature values beforehand reduces the number of potential splits to just the\n",
    "bin edges. Since gradient boosting combines several models, the ensemble size\n",
    "compensates for fewer available splits.\n",
    "\n",
    "Let's see how to bin a dataset using scikit-learn's `KBinsDiscretizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0bb3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "discretizer = KBinsDiscretizer(n_bins=10, encode=\"ordinal\", strategy=\"uniform\")\n",
    "X_trans = discretizer.fit_transform(X)\n",
    "X_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b1a5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(np.unique(col)) for col in X_trans.T]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68587cd4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "\n",
    "We use 10 bins for each feature.\n",
    "\n",
    "**EXERCISE:**\n",
    "\n",
    "1. Create a pipeline with a `KBinsDiscretizer` followed by a\n",
    "   `GradientBoostingClassifier`.\n",
    "2. Compare its training time to a vanilla `GradientBoostingClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be701a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddda0c1",
   "metadata": {},
   "source": [
    "\n",
    "Scikit-learn provides `HistGradientBoostingClassifier`, an approximate\n",
    "gradient boosting algorithm similar to `lightgbm` and `xgboost`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b33d905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "clf = HistGradientBoostingClassifier(max_iter=200, max_bins=10)\n",
    "\n",
    "start = time.time()\n",
    "clf.fit(X_train, y_train)\n",
    "end = time.time()\n",
    "print(f\"Training time: {end - start:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f4d0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e65c06b",
   "metadata": {},
   "source": [
    "\n",
    "### Hyperparameters\n",
    "\n",
    "Gradient boosting couples its parameters, so we must set them together. The key\n",
    "parameters include `n_estimators`, `max_depth`, and `learning_rate`.\n",
    "\n",
    "The `max_depth` parameter matters because gradient boosting fits the error\n",
    "of previous trees. Full-grown trees harm performance - the first tree would\n",
    "overfit the data, leaving no residuals for subsequent trees. Trees in\n",
    "gradient boosting work best with low depth (3-8 levels). Weak learners at\n",
    "each step reduce overfitting.\n",
    "\n",
    "Deeper trees correct residuals faster, requiring fewer learners. Thus,\n",
    "lower `max_depth` values need more `n_estimators`.\n",
    "\n",
    "The `learning_rate` parameter controls how aggressively trees correct errors.\n",
    "Low learning rates correct residuals for fewer samples. High rates (e.g., 1)\n",
    "correct residuals for all samples. Very low learning rates need more\n",
    "estimators, while high rates risk overfitting like deep trees.\n",
    "\n",
    "The next chapter covers finding optimal hyperparameter combinations.\n",
    "\n",
    "The `early_stopping` parameter helps in histogram gradient boosting. It\n",
    "splits data during `fit` and uses a validation set to measure improvement\n",
    "from new trees. If new estimators stop improving performance, fitting stops.\n",
    "Let's see this in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc666b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HistGradientBoostingClassifier(early_stopping=True, max_iter=1_000)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819bb42d",
   "metadata": {},
   "source": [
    "\n",
    "We requested 1,000 trees - more than needed. Let's check how many trees\n",
    "the model actually used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396d098d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.n_iter_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd916c6",
   "metadata": {},
   "source": [
    "\n",
    "The gradient boosting stopped after 127 trees, determining additional trees\n",
    "would not improve performance."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
